{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"NYC_ETA\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all data parquet files\n",
    "\n",
    "jul_data_path = \"../data/yellow_tripdata_2024-07.parquet\"\n",
    "aug_data_path = \"../data/yellow_tripdata_2024-08.parquet\"\n",
    "sep_data_path = \"../data/yellow_tripdata_2024-09.parquet\"\n",
    "df_jul = spark.read.parquet(jul_data_path)\n",
    "df_aug = spark.read.parquet(aug_data_path)\n",
    "df_sep = spark.read.parquet(sep_data_path)\n",
    "\n",
    "df = df_jul.union(df_aug).union(df_sep)\n",
    "\n",
    "output_path = \"../data/merged_yellow_tripdata_2024_Q3.parquet\"\n",
    "# df.write.parquet(output_path)\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data_schema.png](./data_schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Taxi Zone Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_df = spark.read.csv(\"../data/taxi_zone_lookup.csv\", header=True, inferSchema=True)\n",
    "zones_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge **zones_df** with **df** by **Pick Up Location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(zones_df, df.PULocationID == zones_df.LocationID, \"inner\")\n",
    "\n",
    "df = df \\\n",
    "    .withColumnRenamed(\"Borough\", \"PU_Borough\") \\\n",
    "    .withColumnRenamed(\"Zone\", \"PU_Zone\") \\\n",
    "    .withColumnRenamed(\"service_zone\", \"PU_service_zone\")\n",
    "\n",
    "df = df.drop(\"LocationID\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge **zones_df** with **df** by **Drop Off Location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(zones_df, df.DOLocationID == zones_df.LocationID, \"inner\")\n",
    "\n",
    "df = df \\\n",
    "    .withColumnRenamed(\"Borough\", \"DO_Borough\") \\\n",
    "    .withColumnRenamed(\"Zone\", \"DO_Zone\") \\\n",
    "    .withColumnRenamed(\"service_zone\", \"DO_service_zone\")\n",
    "\n",
    "df = df.drop(\"LocationID\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop some irrelevant columns which has no or little impact in estimate time arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    \"store_and_fwd_flag\", \n",
    "    \"payment_type\", \n",
    "    \"fare_amount\", \n",
    "    \"extra\", \n",
    "    \"mta_tax\", \n",
    "    \"tip_amount\", \n",
    "    \"tolls_amount\", \n",
    "    \"improvement_surcharge\", \n",
    "    \"total_amount\", \n",
    "    \"PU_service_zone\", \n",
    "    \"DO_service_zone\"\n",
    "]\n",
    "\n",
    "df = df.drop(*columns_to_drop)\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trip_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, mean as spark_mean, stddev\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "trip_dis_stats = df.select(\n",
    "    spark_mean(col(\"trip_distance\")).alias(\"mean\"),\n",
    "    stddev(col(\"trip_distance\")).alias(\"stddev\")\n",
    ").collect()[0]\n",
    "trip_dis_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_dis_mean = trip_dis_stats[\"mean\"]\n",
    "trip_dis_stddev = trip_dis_stats[\"stddev\"]\n",
    "\n",
    "print(f\"Mean for Trip Duration is: {trip_dis_mean}\")\n",
    "print(f\"Standard Deviation for Trip Duration is: {trip_dis_stddev}\")\n",
    "\n",
    "# Filter rows within 2 standard deviations\n",
    "df = df.filter(\n",
    "    (col(\"trip_distance\") >= trip_dis_mean - 2 * trip_dis_stddev) &\n",
    "    (col(\"trip_distance\") <= trip_dis_mean + 2 * trip_dis_stddev) &\n",
    "    (col(\"trip_distance\") >= 1)\n",
    ")\n",
    "\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_distance_data = df.select(\"trip_distance\").toPandas()\n",
    "\n",
    "trip_distance_data = trip_distance_data[trip_distance_data['trip_distance'] > 0]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(np.log(trip_distance_data['trip_distance']), bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of Log-transformed Trip Distance')\n",
    "plt.xlabel('Log of Trip Distance (miles)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(np.log(trip_distance_data['trip_distance']), vert=False)\n",
    "plt.title('Boxplot of Log-transformed Trip Distance')\n",
    "plt.xlabel('Log of Trip Distance (miles)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### passenger_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter rows where `passenger_count` is 7, 8, 9 or 0 because it is uncommon in NYC taxi data\n",
    "> *(most taxis accommodate up to 6 passengers)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df = df.filter(~(col(\"passenger_count\").isin([7, 8, 9, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### airport_fee to enter_airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"enter_airport\", \n",
    "    when(col(\"airport_fee\") > 0, 1).otherwise(0)\n",
    ")\n",
    "df.select(\"enter_airport\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"airport_fee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pick_up and drop_off datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df.withColumn(\"tpep_pickup_datetime\", to_timestamp(\"tpep_pickup_datetime\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "       .withColumn(\"tpep_dropoff_datetime\", to_timestamp(\"tpep_dropoff_datetime\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "df.select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tpep_pickup_datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn(\"pickup_date\", F.to_date(F.col(\"tpep_pickup_datetime\")))\n",
    "df = df.withColumn(\"pickup_hour\", F.hour(F.col(\"tpep_pickup_datetime\")))\n",
    "df = df.withColumn(\"pickup_minute\", F.minute(F.col(\"tpep_pickup_datetime\")))\n",
    "df = df.withColumn(\"pickup_second\", F.second(F.col(\"tpep_pickup_datetime\")))\n",
    "df = df.withColumn(\"pickup_weekday\", F.dayofweek(F.col(\"tpep_pickup_datetime\")) - 1)  # Adjust to 0-based (Sunday: 0, Monday: 1,...)\n",
    "df = df.withColumn(\"pickup_month\", F.month(F.col(\"tpep_pickup_datetime\")))\n",
    "df = df.withColumn(\"pickup_day\", F.day(F.col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "\n",
    "# Weekly hour feature (captures patterns based on the time of day and the day of the week)\n",
    "df = df.withColumn(\n",
    "    \"pickup_week_hour\", F.col(\"pickup_weekday\") * 24 + F.col(\"pickup_hour\")  # 0 (Sunday midnight) to 167 (Saturday 11:00 PM)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"pickup_time\", \n",
    "    F.concat(\n",
    "        F.lpad(F.col(\"pickup_hour\"), 2, \"0\"), F.lit(\":\"),\n",
    "        F.lpad(F.col(\"pickup_minute\"), 2, \"0\"), F.lit(\":\"),\n",
    "        F.lpad(F.col(\"pickup_second\"), 2, \"0\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tpep_dropoff_datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"dropoff_date\", F.to_date(F.col(\"tpep_dropoff_datetime\")))\n",
    "df = df.withColumn(\"dropoff_hour\", F.hour(F.col(\"tpep_dropoff_datetime\")))\n",
    "df = df.withColumn(\"dropoff_minute\", F.minute(F.col(\"tpep_dropoff_datetime\")))\n",
    "df = df.withColumn(\"dropoff_second\", F.second(F.col(\"tpep_dropoff_datetime\")))\n",
    "df = df.withColumn(\"dropoff_weekday\", F.dayofweek(F.col(\"tpep_dropoff_datetime\")) - 1)  # Adjust to 0-based (Sunday: 0, Monday: 1,...)\n",
    "df = df.withColumn(\"dropoff_month\", F.month(F.col(\"tpep_dropoff_datetime\")))\n",
    "df = df.withColumn(\"dropoff_day\", F.day(F.col(\"tpep_dropoff_datetime\")))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"dropoff_time\", \n",
    "    F.concat(\n",
    "        F.lpad(F.col(\"dropoff_hour\"), 2, \"0\"), F.lit(\":\"),\n",
    "        F.lpad(F.col(\"dropoff_minute\"), 2, \"0\"), F.lit(\":\"),\n",
    "        F.lpad(F.col(\"dropoff_second\"), 2, \"0\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df.select(\n",
    "    \"pickup_date\", \"pickup_time\" , \"pickup_month\", \"pickup_day\", \"pickup_weekday\", \"pickup_hour\", \"pickup_minute\", \"pickup_second\", \n",
    "    \"pickup_week_hour\", \"dropoff_date\", \"dropoff_time\" , \"dropoff_month\", \"dropoff_day\", \"dropoff_hour\", \"dropoff_minute\", \"dropoff_second\"\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"trip_duration_seconds\", \n",
    "    (F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\")) \n",
    ")\n",
    "df = df.filter(col(\"trip_duration_seconds\") <= 100000)\n",
    "df.select(\"trip_duration_seconds\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### speed in miles per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"speed_mph\", F.col(\"trip_distance\") / (F.col(\"trip_duration_seconds\") / 3600))\n",
    "min_speed = 0\n",
    "max_speed = 100\n",
    "\n",
    "df = df.filter((F.col(\"speed_mph\") >= min_speed) & (F.col(\"speed_mph\") <= max_speed))\n",
    "df.select(\"speed_mph\").describe().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rows with IQR of speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the speed_mph column to a pandas DataFrame for visualization\n",
    "speed_data = df.select(\"speed_mph\").sample(fraction=0.1, seed=42).toPandas()\n",
    "\n",
    "# Plot the distribution of speed\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(speed_data['speed_mph'], bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of Speed (mph)')\n",
    "plt.xlabel('Speed (mph)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot the boxplot of speed\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(speed_data['speed_mph'], vert=False)\n",
    "plt.title('Boxplot of Speed (mph)')\n",
    "plt.xlabel('Speed (mph)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the IQR for speed_mph\n",
    "Q1_speed = df.approxQuantile(\"speed_mph\", [0.25], 0.05)[0]\n",
    "Q3_speed = df.approxQuantile(\"speed_mph\", [0.75], 0.05)[0]\n",
    "IQR_speed = Q3_speed - Q1_speed\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound_speed = Q1_speed - 1.5 * IQR_speed\n",
    "upper_bound_speed = Q3_speed + 1.5 * IQR_speed\n",
    "\n",
    "# Filter the dataframe to remove outliers\n",
    "df = df.filter((col(\"speed_mph\") >= lower_bound_speed) & (col(\"speed_mph\") <= upper_bound_speed))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Total trip duration throughout a day every day in July"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "july_data = df.filter(F.col(\"pickup_month\") == 7)\n",
    "\n",
    "july_data = july_data.groupBy(\"pickup_day\").agg(\n",
    "    F.sum(\"trip_duration_seconds\").alias(\"total_trip_duration_seconds\"),\n",
    "    F.count(\"pickup_day\").alias(\"trip_count\")\n",
    ")\n",
    "\n",
    "july_data = july_data.orderBy(\"pickup_day\")\n",
    "\n",
    "pandas_july_data = july_data.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(pandas_july_data['pickup_day'], pandas_july_data['total_trip_duration_seconds'], linestyle='-', marker='o', color='b', alpha=0.6)\n",
    "\n",
    "plt.title('Total Trip Duration in July by Day of the Month', fontsize=12)\n",
    "plt.xlabel('Day of the Month', fontsize=8)\n",
    "plt.ylabel('Total Trip Duration (Seconds)', fontsize=8)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total number of trips every weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by pickup_weekday and count the number of trips\n",
    "weekday_trip_count = df.groupBy(\"pickup_weekday\").agg(F.count(\"pickup_weekday\").alias(\"trip_count\"))\n",
    "\n",
    "# Order by pickup_weekday\n",
    "weekday_trip_count = weekday_trip_count.orderBy(\"pickup_weekday\")\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "pandas_weekday_trip_count = weekday_trip_count.toPandas()\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(pandas_weekday_trip_count['pickup_weekday'], pandas_weekday_trip_count['trip_count'], color='b', alpha=0.7)\n",
    "\n",
    "plt.title('Number of Trips by Weekday', fontsize=12)\n",
    "plt.xlabel('Weekday (0=Sunday, 6=Saturday)', fontsize=9)\n",
    "plt.ylabel('Number of Trips', fontsize=9)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the number of trips occur in each hour throughout a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_trip_count = df.groupBy(\"pickup_hour\").agg(\n",
    "    F.count(\"pickup_hour\").alias(\"trip_count\")\n",
    ")\n",
    "\n",
    "hourly_trip_count = hourly_trip_count.orderBy(\"pickup_hour\")\n",
    "\n",
    "pandas_hourly_trip_count = hourly_trip_count.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(pandas_hourly_trip_count['pickup_hour'], pandas_hourly_trip_count['trip_count'], color='b', alpha=0.7)\n",
    "\n",
    "plt.title('Number of Trips by Hour of the Day', fontsize=12)\n",
    "plt.xlabel('Hour of the Day', fontsize=9)\n",
    "plt.ylabel('Number of Trips', fontsize=9)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Borough having the most number of trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_borough_counts = df.groupBy(\"PU_Borough\").count()\n",
    "dropoff_borough_counts = df.groupBy(\"DO_Borough\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_borough_counts_pd = pickup_borough_counts.toPandas()\n",
    "dropoff_borough_counts_pd = dropoff_borough_counts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax[0].bar(pickup_borough_counts_pd['PU_Borough'], pickup_borough_counts_pd['count'], color='green', alpha=0.7, label='Pickup Boroughs')\n",
    "ax[0].set_xlabel('Pickup Borough')\n",
    "ax[0].set_ylabel('Number of Trips')\n",
    "ax[0].set_title('Popularity of Pickup Boroughs')\n",
    "ax[0].tick_params(axis='x', rotation=45)\n",
    "ax[0].legend(loc='upper right')\n",
    "\n",
    "ax[1].bar(dropoff_borough_counts_pd['DO_Borough'], dropoff_borough_counts_pd['count'], color='blue', alpha=0.7, label='Dropoff Boroughs')\n",
    "ax[1].set_xlabel('Dropoff Borough')\n",
    "ax[1].set_ylabel('Number of Trips')\n",
    "ax[1].set_title('Popularity of Dropoff Boroughs')\n",
    "ax[1].tick_params(axis='x', rotation=45)\n",
    "ax[1].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Covariance between Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "    \"passenger_count\", \"trip_distance\", \"congestion_surcharge\",\n",
    "    \"pickup_hour\", \"pickup_minute\", \"trip_duration_seconds\", \"speed_mph\"\n",
    "]\n",
    "\n",
    "numeric_df = df.select(numeric_columns).sample(fraction=0.1, seed=42).toPandas()\n",
    "\n",
    "covariance_matrix = numeric_df.cov()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    covariance_matrix,\n",
    "    annot=True, \n",
    "    cmap=\"coolwarm\",\n",
    "    fmt=\".0f\",  \n",
    "    cbar=True,  \n",
    "    square=True, \n",
    "    linewidths=0.5 \n",
    ")\n",
    "plt.title(\"Covariance Heatmap of Numeric Columns\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Weather dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv(\"../data/weather_data/merged_weather.csv\")\n",
    "weather_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = spark.createDataFrame(weather_data)\n",
    "\n",
    "df = df.join(weather_df, (df['pickup_date'] == weather_df['date']) & (df['pickup_hour'] == weather_df['hour']), \"left\")\n",
    "\n",
    "df = df.drop(weather_df['hour'])\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'name', 'date', 'time', 'RatecodeID', 'vendorid', 'store_and_fwd_flag', 'date', 'month', 'time', 'date', 'datetime']\n",
    "\n",
    "df = df.drop(*columns_to_drop)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = df[['average_temperature', 'precipitation', 'snow_fall', 'snow_depth']].sample(fraction=0.1, seed=42).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_duration_seconds_pd = df.select(\"trip_duration_seconds\").sample(fraction=0.1, seed=42).toPandas()\n",
    "\n",
    "for i in scatter.columns:\n",
    "    sns.scatterplot(x=scatter[i], y=trip_duration_seconds_pd['trip_duration_seconds'])\n",
    "    plt.ylabel('Travel Duration (Seconds)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base on the visualization we can drop column snow_fall and snow_depth because observing no noticeable trend or infomation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('snow_fall', 'snow_depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationship between Day Temperature and Number of Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "grouped_data = (\n",
    "    df.groupBy(\"pickup_month\", \"pickup_day\")\n",
    "    .agg(\n",
    "        F.avg(\"average_temperature\").alias(\"avg_temperature\"),\n",
    "        F.count(\"pickup_day\").alias(\"trip_count\")\n",
    "    )\n",
    "    .orderBy(\"pickup_month\", \"pickup_day\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "month_names = {7: \"July\", 8: \"August\", 9: \"September\"}\n",
    "for month in [7, 8, 9]:\n",
    "    month_data = grouped_data[grouped_data[\"pickup_month\"] == month]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=month_data['pickup_day'],\n",
    "        y=month_data['avg_temperature'],\n",
    "        mode='lines',\n",
    "        line=dict(color='orange', width=2),\n",
    "        name='Average Temperature'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=month_data['pickup_day'],\n",
    "        y=month_data['trip_count'],\n",
    "        mode='lines',\n",
    "        line=dict(color='blue', width=2),\n",
    "        name='Trip Count',\n",
    "        yaxis='y2'  # Use secondary y-axis\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Trip Count and Average Temperature in {month_names[month]}\",\n",
    "        xaxis=dict(title=\"Day of Month\"),\n",
    "        yaxis=dict(title=\"Average Temperature (Â°C)\", side=\"left\"),\n",
    "        yaxis2=dict(title=\"Trip Count\", overlaying='y', side=\"right\"),\n",
    "        legend=dict(title=\"Legend\"),\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationship between `prepcipitation` and `trip_count` vs `trip_duration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "precipitation_data = (\n",
    "    df.groupBy(\"precipitation\")\n",
    "    .agg(\n",
    "        F.count(\"pickup_day\").alias(\"trip_count\"),\n",
    "        F.avg(\"trip_duration_seconds\").alias(\"avg_trip_duration\")\n",
    "    )\n",
    "    .orderBy(\"precipitation\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=precipitation_data['precipitation'],\n",
    "    y=precipitation_data['trip_count'],\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='blue', width=2),\n",
    "    name='Trip Count'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=precipitation_data['precipitation'],\n",
    "    y=precipitation_data['avg_trip_duration'],\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='orange', width=2, dash='dot'),\n",
    "    name='Avg Trip Duration',\n",
    "    yaxis='y2'  \n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Relationship Between Precipitation, Trip Count, and Avg Trip Duration\",\n",
    "    xaxis=dict(title=\"Precipitation (mm)\"),\n",
    "    yaxis=dict(title=\"Trip Count\", side=\"left\"),\n",
    "    yaxis2=dict(title=\"Avg Trip Duration (seconds)\", overlaying='y', side=\"right\"),\n",
    "    legend=dict(title=\"Legend\"),\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* People avoid trips during rainy weather unless necessary, as reflected in the trip count. The sharp drop in trip count supports this behavioral trend.\n",
    "* Longer average trip durations in rainy conditions indicate potential traffic congestion, slower driving speeds, or more cautious travel during rain.\n",
    "* The slight increase in trip count for higher precipitation levels might indicate that some trips are unavoidable, regardless of weather conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the congestion trends in each Zone in NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('congestion_surcharge').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_surcharge_spark = df.groupBy(\"PU_Borough\").agg(F.sum(\"congestion_surcharge\").alias(\"Total_Surcharge\"))\n",
    "\n",
    "borough_surcharge_spark = borough_surcharge_spark.orderBy(F.desc(\"Total_Surcharge\"))\n",
    "\n",
    "borough_surcharge_pandas = borough_surcharge_spark.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(borough_surcharge_pandas['PU_Borough'], borough_surcharge_pandas['Total_Surcharge'], color='skyblue')\n",
    "plt.title('Total Congestion Surcharge by Borough')\n",
    "plt.xlabel('Borough')\n",
    "plt.ylabel('Total Congestion Surcharge')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert `congestion_surcharge` to `congestion_level` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_threshold = -2.5  \n",
    "high_threshold = 2.5\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"congestion_level\",\n",
    "    F.when(df[\"congestion_surcharge\"] <= low_threshold, \"Low\")\n",
    "     .when((df[\"congestion_surcharge\"] > low_threshold) & (df[\"congestion_surcharge\"] < high_threshold), \"Medium\")\n",
    "     .otherwise(\"High\")\n",
    ")\n",
    "\n",
    "df.select(\"congestion_surcharge\", \"congestion_level\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `PU_Zone` that has High `congestion_level`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_congestion_zones = df.filter(F.col(\"congestion_level\") == \"High\") \\\n",
    "    .groupBy(\"PU_Zone\", \"PU_Borough\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "high_congestion_zones.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Touch the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'precipitation' to 'rain' (0 or 1)\n",
    "df = df.withColumn(\"rain\", F.when(df[\"precipitation\"] > 0, 1).otherwise(0))\n",
    "\n",
    "# Convert 'congestion_level' to 0, 1, or 2 (low, medium, high -> 0, 1, 2)\n",
    "df = df.withColumn(\"congestion_level\", \n",
    "                   F.when(df[\"congestion_level\"] == \"Low\", 0)\n",
    "                   .when(df[\"congestion_level\"] == \"Medium\", 1)\n",
    "                   .when(df[\"congestion_level\"] == \"High\", 2)\n",
    "                   .otherwise(-1))  # or set it to None if needed\n",
    "\n",
    "# Show the updated dataframe with the transformed columns\n",
    "df.select(\"precipitation\", \"rain\", \"enter_airport\", \"congestion_level\").show(truncate=False)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop('tpep_pickup_datetime', 'tpep_dropoff_datetime', 'congestion_surcharge', 'maximum_temperature', 'minimum_temperature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_ratio_spark(df):\n",
    "    total_rows = df.count()\n",
    "\n",
    "    missing_counts = []\n",
    "    for column in df.columns:\n",
    "        if dict(df.dtypes)[column] in ['float', 'double']:\n",
    "            missing_count = df.filter(F.col(column).isNull() | F.isnan(column)).count()\n",
    "        else:\n",
    "            missing_count = df.filter(F.col(column).isNull()).count()\n",
    "        \n",
    "        missing_ratio = (missing_count / total_rows) * 100\n",
    "        missing_counts.append((column, missing_ratio))\n",
    "    \n",
    "    missing_data_df = spark.createDataFrame(missing_counts, [\"column_name\", \"missing_ratio\"])\n",
    "\n",
    "    missing_data_df = missing_data_df.orderBy(F.col(\"missing_ratio\"), ascending=False)\n",
    "\n",
    "    #missing_data_df = missing_data_df.filter(F.col(\"missing_ratio\") > 0)\n",
    "    \n",
    "    return missing_data_df\n",
    "\n",
    "missing_data = missing_ratio_spark(df)\n",
    "missing_data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - Test Split and Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spark, test_spark = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Select the features and target columns\n",
    " `trip_duration_seconds` is the target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection by using Spark MLlib with RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine features into a single vector column using VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# List of columns to be indexed\n",
    "string_columns = [\n",
    "\t\"PU_Borough\", \"PU_Zone\", \"DO_Borough\", \"DO_Zone\", \n",
    "\t\"enter_airport\", \"pickup_time\", \"dropoff_time\", \n",
    "\t\"precipitation\", \"congestion_level\"\n",
    "]\n",
    "\n",
    "# Index string columns\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in string_columns]\n",
    "\n",
    "# Apply the indexers\n",
    "for indexer in indexers:\n",
    "\tdf = indexer.fit(df).transform(df)\n",
    "\n",
    "# Update feature columns to include indexed columns and exclude original string columns\n",
    "feature_columns = [col for col in df.columns if col not in string_columns + ['trip_duration_seconds', 'pickup_date', 'dropoff_date']]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "df_assembled = assembler.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembled.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Random Forest model to get feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol='features', labelCol='trip_duration_seconds')\n",
    "model = rf.fit(df_assembled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.featureImportances\n",
    "feature_importance_dict = dict(zip(feature_columns, importances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort features by importance and select top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature importance sorted by importance:\")\n",
    "for feature, importance in sorted_feature_importances:\n",
    "    print(f'{feature}: {importance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select top k important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_features = [feature for feature, _ in sorted_feature_importances[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the DataFrame to use only the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_selected = VectorAssembler(inputCols=top_k_features, outputCol='selected_features')\n",
    "df_selected = assembler_selected.transform(df)\n",
    "\n",
    "df_selected.select('selected_features', 'trip_duration').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
